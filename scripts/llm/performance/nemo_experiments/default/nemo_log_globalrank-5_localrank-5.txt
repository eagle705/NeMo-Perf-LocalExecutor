[NeMo W 2025-03-12 23:59:47 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-12 23:59:47 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-12 23:59:47 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-12 23:59:51 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-12 23:59:51 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
[NeMo I 2025-03-12 23:59:51 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-13 00:05:55 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 00:05:55 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 00:05:55 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 00:05:59 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-13 00:05:59 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
[NeMo I 2025-03-13 00:05:59 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-13 00:35:33 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 00:35:33 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 00:35:33 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 00:35:33 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 00:35:34 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 00:35:37 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 00:35:37 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 00:35:37 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 00:35:37 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 1382096896
[NeMo W 2025-03-13 00:43:12 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 00:43:12 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 00:43:12 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 00:43:16 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 00:43:16 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 00:43:16 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 00:43:16 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 1382096896
[NeMo W 2025-03-13 01:05:47 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 01:05:47 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:05:47 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 01:05:51 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 01:05:51 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 01:05:51 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-13 01:15:48 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 01:15:48 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:15:48 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 01:15:54 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 01:15:54 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 01:15:54 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-13 01:19:42 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 01:19:42 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:19:42 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 01:19:45 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 01:19:45 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 01:19:45 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 01:19:45 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 954245120
[NeMo W 2025-03-13 01:26:49 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 01:26:49 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:26:49 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 01:26:53 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 01:26:53 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 01:26:53 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 01:26:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 954245120
[NeMo W 2025-03-13 01:28:44 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 01:28:44 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:28:44 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 01:28:49 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 01:28:49 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 01:28:49 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 01:28:49 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 954245120
[NeMo W 2025-03-13 01:28:49 nemo_logging:405] Tensor parallel overlap: No overlap config provided. Initializing TP comm overlap with the default config.
[NeMo W 2025-03-13 01:37:34 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 01:37:34 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:37:34 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 01:37:38 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 01:37:38 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 01:37:38 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 01:37:38 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 954245120
[NeMo W 2025-03-13 01:38:11 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo I 2025-03-13 01:45:59 nemo_logging:393] Running garbage collection at train global_step: 100
[NeMo W 2025-03-13 01:53:53 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 01:53:53 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:53:53 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 01:53:57 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 01:53:57 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 01:53:57 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 01:53:57 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 17640464384
[NeMo W 2025-03-13 01:56:24 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 01:56:24 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has tensor model parallel rank: 5
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:56:24 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 01:56:29 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 01:56:29 nemo_logging:393] Padded vocab_size: 129024, original vocab_size: 128256, dummy tokens: 768.
[NeMo I 2025-03-13 01:56:30 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 01:56:30 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (5 ,0): 8821940224
[NeMo W 2025-03-13 01:59:30 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 01:59:30 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 01:59:30 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 01:59:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 01:59:31 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 01:59:34 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 01:59:34 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 01:59:34 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 01:59:34 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 9083428864
[NeMo W 2025-03-13 02:00:44 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-13 02:10:10 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 02:10:10 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:10:10 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 02:10:14 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 02:10:14 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 02:10:14 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 02:10:14 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 9083428864
[NeMo W 2025-03-13 02:11:24 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-13 02:15:59 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 02:15:59 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:15:59 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 02:16:03 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 02:16:03 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 02:16:03 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 02:16:03 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 9083428864
[NeMo W 2025-03-13 02:17:13 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-13 02:40:06 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 02:40:06 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 02:40:06 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 02:40:06 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:40:07 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 02:40:13 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo W 2025-03-13 02:42:33 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 02:42:33 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:42:33 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 02:42:37 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo W 2025-03-13 02:47:40 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 02:47:40 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 02:47:40 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 02:47:46 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 02:47:46 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 02:47:46 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 02:47:46 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 5660614656
[NeMo W 2025-03-13 02:48:33 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-13 03:19:50 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 03:19:50 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 03:19:50 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 03:19:50 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 03:19:51 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 03:19:54 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 03:19:54 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 03:19:55 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 03:19:55 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 5660614656
[NeMo W 2025-03-13 03:20:33 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-13 04:02:56 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 04:02:56 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:02:56 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 04:03:00 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 1
[NeMo I 2025-03-13 04:03:00 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-13 04:03:01 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-13 04:03:01 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 5660614656
[NeMo W 2025-03-13 04:03:57 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-13 04:11:25 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 04:11:25 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 04:11:25 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 04:11:25 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:11:26 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 04:11:30 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-13 04:11:30 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
[NeMo I 2025-03-13 04:11:30 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-13 04:14:05 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 04:14:05 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:14:05 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 04:14:09 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-13 04:14:09 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
[NeMo I 2025-03-13 04:14:09 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-13 04:14:43 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-13 04:16:00 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 04:16:00 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:16:00 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 04:16:04 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-13 04:16:04 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
[NeMo I 2025-03-13 04:16:04 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-13 04:16:37 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-13 04:17:46 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 04:17:46 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 04:17:46 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 04:17:46 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:17:47 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 04:17:51 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-13 04:17:51 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
[NeMo I 2025-03-13 04:17:51 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-13 04:18:22 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-13 04:24:07 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-13 04:24:07 nemo_logging:393] Experiments will be logged at /work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-13 04:24:07 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/code/nemo-perf-recipe/NeMo_2502/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-13 04:24:07 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-13 04:24:08 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-13 04:24:11 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-13 04:24:11 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
[NeMo I 2025-03-13 04:24:12 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-13 04:24:40 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-18 06:12:23 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 06:12:23 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has context parallel group: [4, 5]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:12:23 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 06:12:27 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 32
[NeMo I 2025-03-18 06:12:27 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.
[NeMo I 2025-03-18 06:12:27 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-18 06:14:01 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 06:14:01 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 06:14:01 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 06:14:01 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:14:02 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-03-18 06:20:15 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 06:20:15 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has context parallel group: [4, 5]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:20:15 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 06:20:20 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 32
[NeMo I 2025-03-18 06:20:20 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.
[NeMo I 2025-03-18 06:20:20 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-18 06:20:45 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-18 06:31:20 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 06:31:20 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has context parallel group: [4, 5]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:31:20 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 06:31:24 nemo_logging:393] Set CUDA_DEVICE_MAX_CONNECTIONS to 32
[NeMo I 2025-03-18 06:31:24 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.
[NeMo I 2025-03-18 06:31:24 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-18 06:37:25 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 06:37:25 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:37:25 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 06:37:29 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-18 06:37:29 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.
[NeMo I 2025-03-18 06:37:29 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-18 06:37:56 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-18 06:40:09 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 06:40:09 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 06:40:09 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 06:40:09 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:40:10 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 06:40:13 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-18 06:40:13 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.
[NeMo I 2025-03-18 06:40:14 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-18 06:40:49 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo I 2025-03-18 06:51:46 nemo_logging:393] Running garbage collection at train global_step: 100
[NeMo W 2025-03-18 06:52:56 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 06:52:56 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 06:52:56 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 06:53:02 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-18 06:53:02 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.
[NeMo I 2025-03-18 06:53:03 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-18 06:53:19 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo I 2025-03-18 07:01:48 nemo_logging:393] Running garbage collection at train global_step: 100
[NeMo W 2025-03-18 07:02:55 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 07:02:55 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:02:55 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 07:02:59 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-18 07:02:59 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-18 07:02:59 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-18 07:02:59 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 5660614656
[NeMo W 2025-03-18 07:03:35 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-18 07:35:51 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 07:35:51 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:35:51 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 07:35:55 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-18 07:35:55 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-18 07:35:56 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-18 07:35:56 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 5660614656
[NeMo W 2025-03-18 07:36:28 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-18 07:41:26 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 07:41:26 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 07:41:26 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 07:41:26 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:41:28 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 07:41:31 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-18 07:41:31 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
[NeMo I 2025-03-18 07:41:31 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-18 07:41:56 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-18 07:44:09 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 07:44:09 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:44:09 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 07:44:15 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-18 07:44:15 nemo_logging:393] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
[NeMo I 2025-03-18 07:44:16 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo W 2025-03-18 07:44:38 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-18 07:52:24 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 07:52:24 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 07:52:25 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:52:25 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 07:52:29 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-18 07:52:29 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-18 07:52:29 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-18 07:52:29 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 5660614656
[NeMo W 2025-03-18 07:53:04 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
[NeMo W 2025-03-18 07:58:16 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
[NeMo I 2025-03-18 07:58:16 nemo_logging:393] Experiments will be logged at /work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default
[NeMo W 2025-03-18 07:58:16 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/work/NeMo-Perf-LocalExecutor/scripts/llm/performance/nemo_experiments/default/checkpoints. Training from scratch.
[NeMo I 2025-03-18 07:58:16 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-03-18 07:58:17 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-03-18 07:58:19 nemo_logging:393] Unset CUDA_DEVICE_MAX_CONNECTIONS
[NeMo I 2025-03-18 07:58:19 nemo_logging:393] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.
[NeMo I 2025-03-18 07:58:20 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.
[NeMo I 2025-03-18 07:58:20 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 5660614656
[NeMo W 2025-03-18 07:58:48 rerun_state_machine:1088] Implicit initialization of Rerun State Machine!
